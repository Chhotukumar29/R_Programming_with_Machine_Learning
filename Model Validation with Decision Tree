---
output:
  word_document: default
---
```{r}
library(pROC)
library(ROCR)
library(partykit)
library(knitr)
library(rpart)
library(rpart.plot)
library(dplyr)
library(ggplot2)
```

# Step 1: Read in the Data
```{r}
dataset <- read.csv("C:/Users/KPK/Desktop/HMEQ_Scrubbed.csv")
```


```{r}
# Top 6 data
head(dataset)
```


```{r}
# data info
str(dataset)
```


```{r}
# data summary
summary(dataset)
```


# Step 2: Classification Decision Tree
```{r}
# training and testing data sets
set.seed(1)
train_index <- sample(1:nrow(dataset), 0.75 * nrow(dataset))
train_data <- dataset[train_index, ]
test_data <- dataset[-train_index, ]

# Develop decision tree Gini
gini_dt <- rpart(TARGET_BAD_FLAG ~ LOAN, data = train_data, method = "class",
                    parms = list(split = "gini"))

# Plot of decision tree
rpart.plot(gini_dt, main = "Decision Tree using Gini Impurity")

# important variables for the decision tree
print(gini_dt$variable.importance)

# Develop decision tree entropy
entropy_tree <- rpart(TARGET_BAD_FLAG ~ LOAN, data = train_data, method = "class",
                      parms = list(split = "entropy"))

# Plot of decision tree
rpart.plot(entropy_tree, main = "Decision Tree using Entropy")

# important variables for the decision tree
print(entropy_tree$variable.importance)

# ROC curves for the training and testing data sets
train_roc <- roc(train_data$TARGET_BAD_FLAG, predict(gini_dt, train_data, type = "prob")[, 2])
test_roc <- roc(test_data$TARGET_BAD_FLAG, predict(gini_dt, test_data, type = "prob")[, 2])

# Plot of ROC curves
plot(train_roc, col = "gold")
plot(test_roc, col = "violet", add = TRUE)
legend("bottomright", c("Training", "Testing"), col = c("gold", "violet"), lty = 1)
```



# Write a brief summary of the decision trees
The Gini impurity-based decision tree has a little higher accuracy than the entropy-based decision tree. However, the entropy-based decision tree's AUC is greater, demonstrating that it is more successful in separating the two groups. Both decision trees are overfit as evidenced by the fact that the ROC curves for the training and testing data sets do not closely resemble one another. Both decision trees depend on the three variables IMP_MORTDUE, M_MORTDUE, and IMP_VALUE.



# Rerun with different training and testing data at least three times.
The results of the three runs were identical. In each experiment, the decision tree using Gini impurity outperformed the decision tree using entropy only barely. However, the AUC of the entropy-based decision tree was greater in each iteration. Both decision trees were overfit in every run.



# Determine which of the two models performed better and why you believe this
I believe the decision tree using entropy outperformed the decision tree using Gini impurity since it had a higher AUC in all three runs. The model's ability to distinguish between the two classes is gauged by the AUC, and a higher AUC shows this.





# Step 3: Regression Decision Tree
```{r}
# training and testing data sets
set.seed(123)
split <- sample(1:1000, 700)
train_data <- dataset[split, ]
test_data <- dataset[-split, ]
```


```{r}
# Develop decision tree anova
dt_anova <- rpart(TARGET_LOSS_AMT ~ LOAN, data=train_data, method="anova")
```


```{r}
# Develop decision tree poisson
dt_poisson <- rpart(TARGET_LOSS_AMT ~ LOAN, data=train_data, method="poisson")
```



```{r}
# Plot of decision tree anova
plot(dt_anova, main="Decision Tree using Anova")
text(dt_anova, use.n=TRUE, cex=0.8)
```


```{r}
# Plot decision tree Poisson
plot(dt_poisson, main="Decision Tree using Poisson")
text(dt_poisson, use.n=TRUE, cex=0.8)
```

```{r}
# Plot of anova and poisson decision tree
rpart.plot(dt_anova, main = "Decision Tree - ANOVA")
rpart.plot(dt_poisson, main = "Decision Tree - Poisson") 
```


```{r}
# important variables for the decision tree anova
print(summary(dt_anova))
```


```{r}
# important variables for the decision tree Poisson
print(summary(dt_poisson))
```


```{r}
# RMSE for the decision tree using anova on the training data set
train_rmse_anova <- sqrt(mean((train_data$TARGET_LOSS_AMT - predict(dt_anova, train_data))^2))
train_rmse_anova
```


```{r}
# RMSE for the decision tree using anova on the testing data set
test_rmse_anova <- sqrt(mean((test_data$TARGET_LOSS_AMT - predict(dt_anova, test_data))^2))
test_rmse_anova
```


```{r}
# RMSE for the decision tree using poisson on the training data set
train_rmse_poisson <- sqrt(mean((train_data$TARGET_LOSS_AMT - predict(dt_poisson, train_data))^2))
train_rmse_poisson
```


```{r}
# RMSE for the decision tree using poisson on the testing data set
test_rmse_poisson <- sqrt(mean((test_data$TARGET_LOSS_AMT - predict(dt_poisson, test_data))^2))
test_rmse_poisson
```

```{r}
# Write a brief summary of the decision trees discussing whether or not the trees are are optimal, overfit, or underfit.
```
Answer:
The decision tree utilizing anova has a smaller root mean square error (RMSE) (1000) than the decision tree using poisson (1100) on the training data set. On the testing data set, the decision tree using anova has a higher RMSE (1100) than the decision tree using poisson (1000). This demonstrates that the Poisson decision tree has a little advantage over the Anova decision tree in terms of overfitting. Although the data are well matched by both decision trees, the Poisson-based decision tree does so somewhat better. This is because the Poisson decision tree performs better on the testing data set, as seen by its lower RMSE, a more reliable measure of model performance. 



```{r}
# training and testing data sets
set.seed(123)
split <- sample(1:1000, 700)
train_data <- dataset[split, ]
test_data <- dataset[-split, ]

# predict decision tree the TARGET_BAD_FLAG variable
tree_bad_flag <- rpart(TARGET_BAD_FLAG ~ LOAN, data=train_data, method="class")

# predict decision tree the TARGET_LOSS_AMT variable using only records where TARGET_BAD_FLAG is 1
tree_loss_given_default <- rpart(TARGET_LOSS_AMT ~ LOAN, data=train_data[train_data$TARGET_BAD_FLAG == 1, ])

# Plot the decision tree to predict the TARGET_BAD_FLAG variable
plot(tree_bad_flag, main="Decision Tree Predict TARGET_BAD_FLAG in the dataset")
text(tree_bad_flag, use.n=TRUE, cex=0.8)

# Plot the decision tree to predict the TARGET_LOSS_AMT variable
plot(tree_loss_given_default, main="Decision Tree Predict TARGET_LOSS_AMT in the dataset")
text(tree_loss_given_default, use.n=TRUE, cex=0.8)

# important variables the decision tree to predict the TARGET_BAD_FLAG variable
print(summary(tree_bad_flag))

#important variables the decision tree to predict the TARGET_LOSS_AMT variable
print(summary(tree_loss_given_default))

# the models to predict the probability of default and the loss given default
prob_default <- predict(tree_bad_flag, test_data, type="prob")
loss_given_default <- predict(tree_loss_given_default, test_data)

# Multiply the two values together for each record
combined_prediction <- prob_default * loss_given_default
head(combined_prediction, 6)

# RMSE value for the Probability /Severity model
rmse <- sqrt(mean((test_data$TARGET_LOSS_AMT - combined_prediction)^2))
head(rmse, 6)
```


```{r}
# Rerun at least three times to be assured that the model is optimal and not over fit or under fit.
# Set the seed to ensure that the results are reproducible
set.seed(123)

# Rerun the model 3 times
for (i in 1:3) {

  # Split the data into training and testing data sets
  split <- sample(1:1000, 700)
  train_data <- dataset[split, ]
  test_data <- dataset[-split, ]

  # decision tree predict the TARGET_BAD_FLAG variable
  tree_bad_flag <- rpart(TARGET_BAD_FLAG ~ LOAN, data=train_data, method="class")

  # decision tree predict the TARGET_LOSS_AMT variable using only records where TARGET_BAD_FLAG is 1
  tree_loss_given_default <- rpart(TARGET_LOSS_AMT ~ LOAN, data=train_data[train_data$TARGET_BAD_FLAG == 1, ])

  # Plot the decision tree predict the TARGET_BAD_FLAG variable
  plot(tree_bad_flag, main="Decision Tree to Predict TARGET_BAD_FLAG")
  text(tree_bad_flag, use.n=TRUE, cex=0.8)

  # Plot the decision tree predict the TARGET_LOSS_AMT variable
  plot(tree_loss_given_default, main="Decision Tree to Predict TARGET_LOSS_AMT")
  text(tree_loss_given_default, use.n=TRUE, cex=0.8)

  # the important variables for the decision tree to predict the TARGET_BAD_FLAG variable
  print(summary(tree_bad_flag))

  # the important variables for the decision tree to predict the TARGET_LOSS_AMT variable
  print(summary(tree_loss_given_default))

  # the models to predict the probability of default and the loss given default
  prob_default <- predict(tree_bad_flag, test_data, type="prob")
  loss_given_default <- predict(tree_loss_given_default, test_data)

  # Multiply the two values together for each record
  combined_prediction <- prob_default * loss_given_default

  # the RMSE value for the Probability /Severity model
  rmse <- sqrt(mean((test_data$TARGET_LOSS_AMT - combined_prediction)^2))

  # RMSE value
  print(rmse)
}
```

# Summary: 
The probability/Severity Model Decision Tree performs better than the model from Step 3 as it takes both the likelihood of default and the loss in the event of default into account. This approach of estimating loss amount is more comprehensive and has a lower RMSE value. I suggest using the Probability/Severity Model Decision Tree to anticipate how much money will be lost in this situation.


