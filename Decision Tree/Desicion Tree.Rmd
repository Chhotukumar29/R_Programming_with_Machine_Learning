---
output:
  html_document: default
  word_document: default
---
```{r}
library(dplyr)
library(ggplot2)
library(partykit)
library(knitr) 
library(rpart)
library(rpart.plot) 
```

# Step 1: Read in the Data
```{r}
df <- read.csv("C:/Users/KPK/Desktop/HMEQ_Scrubbed.csv")
```


```{r}
# data info
str(df)
```


```{r}
# summary of data
summary(df)
```


```{r}
# top 6 data
head(df)
```


# Step 2: Classification Decision Tree
```{r}
# Gini 
dt_gini <- rpart(TARGET_BAD_FLAG ~ LOAN, data = df, method = "class", parms = list(split = "gini"))

# Entropy 
dt_entropy <- rpart(TARGET_BAD_FLAG ~ LOAN, data = df, method = "class", parms = list(split = "information"))
```


```{r}
rpart.plot(dt_gini, main="Decision Tree - Gini Method")
rpart.plot(dt_entropy, main="Decision Tree - Entropy Method")
```


```{r}
print(dt_gini)
```


```{r}
print(dt_entropy)
```


```{r}
suppressWarnings({
  
library(pROC)
# Gini tree
predictions_gini <- predict(dt_gini, df, type = "prob")

# Entropy tree
predictions_entropy <- predict(dt_entropy, df, type = "prob") 

# ROC curves
roc_gini <- roc(df$TARGET_BAD_FLAG, predictions_gini[, "1"])
roc_entropy <- roc(df$TARGET_BAD_FLAG, predictions_entropy[, "1"])

# ROC Curves plot
plot(roc_gini, main = "ROC Curve - Gini", col = "black")
lines(roc_entropy, col = "pink")
legend("topleft", legend = c("Gini", "Entropy"), col = c("black", "pink"))
})
```


# Summary: I created a decision tree model with Gini and Entropy and showed the decision tree, list of critical variables, and ROC curves for both Gini and Entropy-based trees. A comprehensive review of model performance, interpretability, and alignment with our specific requirement objectives should be used to make a suggestion for the decision tree model and identify the characteristics of persons who are likely to default on a loan. The ROC Curve is in the upper left corner, and the Gini Entropy is close to zero. This indicates that the model effectively distinguishes between the positive and negative categories.





# Step 3: Regression Decision Tree
```{r}
# ANOVA method of dt
tree_anova <- rpart(TARGET_LOSS_AMT ~ LOAN, data = df, method = "anova")

# Poisson method of dt
tree_poisson <- rpart(TARGET_LOSS_AMT ~ LOAN, data = df, method = "poisson")
```


```{r}
# Result 
print(tree_anova)
```


```{r}
# result
print(tree_poisson) 
```


```{r}
# anova and poisson tree plot
rpart.plot(tree_anova, main = "Decision Tree - ANOVA")
rpart.plot(tree_poisson, main = "Decision Tree - Poisson")
```


```{r}
# ANOVA decision tree
printcp(tree_anova)
```


```{r}
# Poisson decision tree
printcp(tree_poisson)
```


```{r}
# Predictions ANOVA 
predictions_anova <- predict(tree_anova, df)

# Predictions Poisson
predictions_poisson <- predict(tree_poisson, df)

# RMSE ANOVA 
rmse_anova <- sqrt(mean((df$TARGET_LOSS_AMT - predictions_anova)^2))

# RMSE Poisson
rmse_poisson <- sqrt(mean((df$TARGET_LOSS_AMT - predictions_poisson)^2))
```


```{r}
# result 
head(predictions_anova, 50)
```


```{r}
head(predictions_poisson, 50)
```


```{r}
rmse_anova
```


```{r}
rmse_poisson
```



# Summary: Using the LOAN variable, the algorithm constructs a regression decision tree to forecast the TARGET_LOSS_AMT. Because the response variable TARGET_LOSS_AMT is numeric, the decision tree ANOVA approach is employed. There are four terminal nodes in the tree. The initial split is based on the LOAN variable, with loans less than or equal to $10,000 going to the left node and loans larger than $10,000 going to the right node. The second split occurs on the left node, where loans less than or equal to 5000 are identified as belonging to the left leaf node and loans larger than 5000 as belonging to the right leaf node. There are no further splits on the right node. The ANOVA decision tree has an RMSE of 110.22. The ANOVA method When the response variable is not excessively skewed, the ANOVA technique of decision tree is chosen over the Poisson approach. Because the response variable TARGET_LOSS_AMT is not severely skewed in this example, the ANOVA approach is a viable choice.





# Step 4: Probability / Severity Model Decision Tree (Optional Bonus Points)
```{r}
# Decision tree according to TARGET_BAD_FLAG
tree_flag <- rpart(TARGET_BAD_FLAG ~ LOAN, data = df, method = "class")

# Decision tree according to TARGET_LOSS_AMT
tree_loss <- rpart(TARGET_LOSS_AMT ~ LOAN, data = subset(df, TARGET_BAD_FLAG == 1))
```


```{r}
rpart.plot(tree_flag, main = "Decision Tree - TARGET_BAD_FLAG")
rpart.plot(tree_loss, main = "Decision Tree - TARGET_LOSS_AMT")
```



# Summary: If the TARGET_BAD_FLAG is set to yes and the LOAN is higher than or equal to 10000, the target loss percentage is 100%. If the TARGET_BAD_FLAG is not set and the LOAN is less than 10,000, the target loss percentage is 80%. If the TARGET_BAD_FLAG is set to yes and the LOAN is less than 10,000, the target loss percentage is 20%. Each branch of the decision tree represents a separate factor that may be used to anticipate the LOAN. The branches are structured hierarchically, with each branch subdivided into smaller branches. This indicates that the factors are interrelated, and that the decision tree can reflect complicated interactions between the elements and the LOAN.






```{r}
# TARGET_BAD_FLAG decision tree
printcp(tree_flag)

# TARGET_LOSS_AMT decision tree
printcp(tree_loss)
```


```{r}
# Predict probability default
prob_default <- predict(tree_flag, df, type = "prob")

# Predict loss given default
tree_loss <- predict(tree_loss, df, type = "vector")  #  (only for records where TARGET_BAD_FLAG is 1) 
```


```{r}
suppressWarnings({
# Multiply probability and loss 
probability_severity <- prob_default * tree_loss 

# RMSE
rmse_probability_severity <- sqrt(mean((df$TARGET_LOSS_AMT[df$TARGET_BAD_FLAG == 1] - probability_severity)^2))
})
```


```{r}
head(probability_severity, 50)
```


```{r}
rmse_probability_severity
```


# Summary: The decision between the Step 3 and Step 4 models is influenced by the specific aims and needs of your analysis or commercial application. If you want to predict loan defaults, you should concentrate on the model in Step 3. If the goal is to anticipate the according to associated with defaults, the model in Step 4 is more suited.



